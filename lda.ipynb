{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation for persona modelling\n",
    "\n",
    "This jupyter notebook creates a LDA model to approximate human personalities. It does so by creating the document-topic matrix and the word distributions for the topics from ParlAI persona descriptions. The idea was that these topics could be used to describe what a person likes, and then to use similar persons to find more topics the person might enjoy, similar to the Netflix challenge, where instead of movies there are topics. \n",
    "\n",
    "The form of this notebook is from here: This is from here https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "The needed packages. Read the `readme` of the folder if you have issues with gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries used. Use the .yml file to create the conda environment. Check the instructions\n",
    "# for how to create a new kernel choice for gensim to make it work\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "from collections import Counter\n",
    "import pandas\n",
    "import matplotlib\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Functions\n",
    "Here the relevant functions are defined. Can then be used to any text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import the desired corpus, and normalize it (all lower case, small lines removed, no punctuation)\n",
    "def import_text(filename):\n",
    "    documents = []\n",
    "    with open(filename, \"r\") as source:\n",
    "        for line in source:\n",
    "            if len(line) < 3:\n",
    "                continue\n",
    "            line = line.lower()\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "            documents.append(line)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to further preprocess the text for NLP (remove stopwords and small words)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(token)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the document-topic matrix. Dimensions will be #documents x #topics\n",
    "# Relevant later\n",
    "def get_doc_topic(corpus, model):\n",
    "    doc_topic_pair = list()\n",
    "    for doc in corpus:\n",
    "        doc_topic_pair.append(model.__getitem__(doc, eps=0))\n",
    "    doc_topic = list()\n",
    "    for doc in doc_topic_pair:\n",
    "        doc_prob = list()\n",
    "        for i, prob in doc:\n",
    "            doc_prob.append(prob)\n",
    "        doc_topic.append(doc_prob)\n",
    "    return doc_topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses the LDA model to generate a new sentence BOW style from a BOW sentence given to it.\n",
    "# At the moment very inefficient\n",
    "def generate_sentence_from_bow(bow, model, topic_words, dictionary):\n",
    "    gen_sentence = []\n",
    "    i = len(bow)\n",
    "    topics = model.get_document_topics(bow) # Might have to use the whole 400 lenght vector instead\n",
    "    while len(gen_sentence) < i:\n",
    "        for topic_idx, topic_prob in topics:\n",
    "            if topic_prob > np.random.rand():\n",
    "                for idx, word_prob in enumerate(topic_words[topic_idx]):\n",
    "                    if word_prob > np.random.rand():\n",
    "                        gen_sentence.append(dictionary.get(idx))\n",
    "                        break\n",
    "                break\n",
    "    return gen_sentence        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocess\n",
    "All the steps before generating the actual LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports the text\n",
    "documents = import_text(\"personas_with_id_train_both_all.txt\")\n",
    "\n",
    "# Preprocess all the docs\n",
    "processed_docs = list(map(preprocess, documents))\n",
    "\n",
    "# No printing of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPORTANT CELL. Just to print and see stuff worked\n",
    "if do_print:\n",
    "    print(len(documents))\n",
    "    print(documents[5:10])\n",
    "\n",
    "    doc_sample = documents[101]\n",
    "    print('original document: ')\n",
    "    words = []\n",
    "    for word in doc_sample.split(' '):\n",
    "        words.append(word)\n",
    "    print(words)\n",
    "    print('\\n\\n tokenized document: ')\n",
    "    print(preprocess(doc_sample))\n",
    "    \n",
    "    print('\\n\\n First ten tokenized document: ')\n",
    "    print(processed_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gensim dictionary object to change documents for desired shape by the LDA\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    count = 0\n",
    "\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the extreme instances (the persona texts have a lot of like words, those are not needed for this)\n",
    "dictionary.filter_extremes(no_above=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bag of words from the documents (LDA wants the corpus as a BOW)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    ex = 1675\n",
    "    print(bow_corpus[ex])\n",
    "    print(processed_docs[ex])\n",
    "    print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF apparently can also be used to create LDA model.\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    pprint(corpus_tfidf[ex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Training the models\n",
    "Actually training the models, both with BOW and TF-IDF. Doesn't seem to be much of a difference even though BOW is the I guess preferred way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the LDA model with the BOW corpus. Number of topics is a very relevant hyperparameter, passes probably less so\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus, num_topics=10, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the LDA model with the TF-IDF corpus\n",
    "lda_model_tfidf = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    lda_model_tfidf.print_topics()\n",
    "    pprint(lda_model.get_document_topics(bow_corpus[ex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    test_sentence1 = \"I like to cook. I also watch the night sky. I don't like horseback riding\"\n",
    "    test_sentence2 = \"i work with computers. i love drinking coffee in the morning. i brew beer\"\n",
    "    test_bow1 = dictionary.doc2bow(preprocess(test_sentence1))\n",
    "    print(test_bow1)\n",
    "    sent1_topics = lda_model.get_document_topics(test_bow1)\n",
    "    pprint(sent1_topics)\n",
    "\n",
    "    test_bow2 = dictionary.doc2bow(preprocess(test_sentence2))\n",
    "    sent2_topics = lda_model.get_document_topics(test_bow2)\n",
    "    pprint(sent2_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_barnados = False\n",
    "if run_barnados:\n",
    "    barnardos = import_text(\"barnardo_lines.txt\")\n",
    "    processed_barnardos = list(map(preprocess, barnardos))\n",
    "    i = 0\n",
    "    for bar in processed_barnardos:\n",
    "        print(i)\n",
    "        print(lda_model.get_document_topics(dictionary.doc2bow(bar)))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Creating matrices\n",
    "Here the relevant matrices are created. Mainly document - topic and topic - word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the gensim function directly to create the topic-word distribution matrix. \n",
    "# Will be form topic (int) x dictionary (float) where float will be the probability of that word.\n",
    "topic_word_matrix = lda_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    print(topic_word_matrix.shape)\n",
    "    print(topic_word_matrix[3][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the function defined at the topic is used. Generates a matrix of the form document x topic\n",
    "document_topic_matrix = get_doc_topic(bow_corpus, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To numpy format, easier to handle and do stuff to later\n",
    "document_topic_matrix = np.asarray(document_topic_matrix, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    print(document_topic_matrix.shape)\n",
    "    print(document_topic_matrix[ex][241:246])\n",
    "    print(sum(document_topic_matrix[23]))\n",
    "    print(sum(document_topic_matrix[123]))\n",
    "    print(sum(document_topic_matrix[2343]))\n",
    "    print(sum(topic_word_matrix[3]))\n",
    "    print(sum(topic_word_matrix[7]))\n",
    "    print(sum(topic_word_matrix[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the matrix into better form for the FUNK SVD, so values close to zero will be zero.\n",
    "# This should make it so the SVD generates recommendations for those values.\n",
    "thresh = 0.1\n",
    "super_threshold_indices = document_topic_matrix < thresh\n",
    "document_topic_matrix[super_threshold_indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    print(document_topic_matrix[ex][90:97])\n",
    "    print(len(bow_corpus[ex]))\n",
    "    topic_vector = lda_model.get_topic_terms(3)\n",
    "    print(len(topic_vector))\n",
    "    print(dictionary.doc2idx([\"educated\", \"highly\", \"dog\"]))\n",
    "    print(dictionary.get(2470))\n",
    "    print(dictionary.get(56))\n",
    "    print(dictionary.get(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    generated_test = generate_sentence_from_bow(test_bow1, lda_model, topic_word_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    print(test_bow1)\n",
    "    print(lda_model.get_document_topics(test_bow1))\n",
    "    print(generated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Testing with Shakespear\n",
    "The latter section is basically testing the same thing with shakespear since it is smaller but still has lots of text for characters for persona modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the previous relevant steps but with shakespear\n",
    "shakespear = import_text(\"formatted_hamlet.txt\")\n",
    "processed_spear = list(map(preprocess, shakespear))\n",
    "\n",
    "dictionary_spear = gensim.corpora.Dictionary(processed_spear)\n",
    "#dictionary_spear.filter_extremes(no_above=0.3)\n",
    "\n",
    "bow_spear = [dictionary_spear.doc2bow(doc) for doc in processed_spear]\n",
    "\n",
    "spear_model = gensim.models.ldamodel.LdaModel(corpus=bow_spear, num_topics=40, id2word=dictionary_spear, passes=2)\n",
    "spear_tw_matrix = spear_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the whole play with the LDA model\n",
    "generated_corpus = []\n",
    "for bow in bow_spear:\n",
    "    generated_corpus.append(generate_sentence_from_bow(bow, spear_model, spear_tw_matrix, dictionary_spear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    sp_ex = 45\n",
    "    print(generated_corpus[sp_ex])\n",
    "    print(bow_spear[sp_ex])\n",
    "    print(processed_spear[sp_ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the word instances with Counter for histagram (or actually a bar graph)\n",
    "lda_spear_cnt = Counter()\n",
    "proc_spear_cnt = Counter()\n",
    "\n",
    "for sentence in generated_corpus:\n",
    "    for word in sentence:\n",
    "        lda_spear_cnt[word] += 1\n",
    "\n",
    "for sentence in processed_spear:\n",
    "    for word in sentence:\n",
    "        proc_spear_cnt[word] += 1\n",
    "lda_most = lda_spear_cnt.most_common(25)\n",
    "org_most = proc_spear_cnt.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    print(\"lda\")\n",
    "    print(lda_most)\n",
    "    print(\"org\")\n",
    "    print(org_most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter results to dictionary for pandas\n",
    "lda_most_dict = dict(lda_most)\n",
    "\n",
    "# How many times the most common words in lda are in the org\n",
    "org_dict = {}\n",
    "for key in lda_most_dict:\n",
    "    org_dict[key] = proc_spear_cnt[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    df1 = pandas.DataFrame.from_dict(lda_most_dict, orient='index')\n",
    "    ax1 = df1.plot(kind='bar')\n",
    "    ax1.set_ylim(0,220)\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_print:\n",
    "    df2 = pandas.DataFrame.from_dict(org_dict, orient='index')\n",
    "    ax2 = df2.plot(kind='bar')\n",
    "    ax2.set_ylim(0,220)\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Replace persona descriptions with a topic description (WIP)\n",
    "\n",
    "In this section the previously trained LDA model is going to be used to generate topics from training files 4 persona lines, and then replacing those lines with the topic list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromFile = \"sample.txt\"\n",
    "toFile = \"topicAndDescription.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTopicsFromPersonaLines(personaLines):\n",
    "    personaLines = personaLines.translate(str.maketrans('', '', string.punctuation))\n",
    "    personaLines = personaLines.rstrip()\n",
    "    preprocessedPersonaLines = preprocess(personaLines)\n",
    "                \n",
    "    bowPersonaLines = dictionary.doc2bow(preprocessedPersonaLines)\n",
    "    \n",
    "    personaDescriptionInWords = []\n",
    "    topicsFromPersonaLines = lda_model.get_document_topics(bowPersonaLines)\n",
    "    for topic, weight in topicsFromPersonaLines:\n",
    "        showTopicResults = lda_model.show_topic(topic)\n",
    "        for word, percentage in showTopicResults:\n",
    "            personaDescriptionInWords.append(word)\n",
    "    stringPersonaDescriptionInWords = \" \".join(personaDescriptionInWords)\n",
    "    personaAsTopicFile.write(\"topics: \" + stringPersonaDescriptionInWords + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################\n",
    "### TODO ###########\n",
    "####################\n",
    "# Line numbering does not work properly. If writePersonaLinesAlso is false\n",
    "# then line numbers skip. If true the topicLine is kind of an extra.\n",
    "\n",
    "print(\"running..\")\n",
    "personaLines = \"\"\n",
    "yourPersonaLinesCounter = 0\n",
    "partnersPersonaLinesCounter = 0\n",
    "writePersonaLinesAlso = True\n",
    "\n",
    "with open(fromFile, 'r') as personachatFile,\\\n",
    "    open(toFile, 'w') as personaAsTopicFile:\n",
    "    for line in personachatFile: \n",
    "\n",
    "            if \"your persona:\" in line and partnersPersonaLinesCounter == 0:\n",
    "                currentPersonaLine = line.split(\": \")[1].rstrip()\n",
    "                personaLines += currentPersonaLine + \" \"\n",
    "                yourPersonaLinesCounter += 1\n",
    "                if writePersonaLinesAlso:\n",
    "                    personaAsTopicFile.write(line)\n",
    "            \n",
    "            elif \"partner's persona:\" in line and yourPersonaLinesCounter == 0:\n",
    "                currentPersonaLine = line.split(\": \")[1].rstrip()\n",
    "                personaLines += currentPersonaLine + \" \"\n",
    "                partnersPersonaLinesCounter += 1\n",
    "                if writePersonaLinesAlso:\n",
    "                    personaAsTopicFile.write(line)\n",
    "                    \n",
    "            elif ((\"partner's persona:\" in line and yourPersonaLinesCounter != 0) or\n",
    "                (\"your persona:\" in line and partnersPersonaLinesCounter != 0)):\n",
    "                \n",
    "                extractTopicsFromPersonaLines(personaLines)\n",
    "                \n",
    "                if yourPersonaLinesCounter != 0:\n",
    "                    yourPersonaLinesCounter = 0\n",
    "                    partnersPersonaLinesCounter += 1\n",
    "                else:\n",
    "                    partnersPersonaLinesCounter = 0\n",
    "                    yourPersonaLinesCounter += 1\n",
    "                personaLines = \"\"\n",
    "                \n",
    "                currentPersonaLine = line.split(\": \")[1].rstrip()\n",
    "                personaLines += currentPersonaLine + \" \"\n",
    "                if writePersonaLinesAlso:\n",
    "                    personaAsTopicFile.write(line)\n",
    "            elif (\"\\t\" in line and \n",
    "                    (yourPersonaLinesCounter != 0 or \n",
    "                    partnersPersonaLinesCounter != 0)):\n",
    "                \n",
    "                extractTopicsFromPersonaLines(personaLines)\n",
    "                personaLines = \"\"\n",
    "                yourPersonaLinesCounter = 0\n",
    "                partnersPersonaLinesCounter = 0\n",
    "                personaAsTopicFile.write(line)\n",
    "            else:\n",
    "                personaAsTopicFile.write(line)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[wordvalue[0] for wordvalue in lda_model.show_topic(8)]\n",
    "lda_model.get_topic_terms(8)\n",
    "a1 = lda_model.get_document_topics(bow_corpus[125])\n",
    "dictionary.get(56)\n",
    "for topic, weight in a1:\n",
    "    print(topic)\n",
    "a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDA",
   "language": "python",
   "name": "lda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
